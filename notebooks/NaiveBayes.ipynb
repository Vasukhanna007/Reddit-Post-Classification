{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Basic Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits\n",
       "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
       "1   1  Ah yes way could have been :( remember when he...              nba\n",
       "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
       "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
       "4   4  Easy. You use the piss and dry technique. Let ...            funny"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = \"../data/reddit_train.csv\"\n",
    "train = pd.read_csv(train_data)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>honestly buffalo is the correct answer remembe...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ah yes way could have been remember when he wa...</td>\n",
       "      <td>nba</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>http youtu be 6xxbbr8isz0 40m49s if you didn f...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>he wouldn have been bad signing if we wouldn h...</td>\n",
       "      <td>soccer</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>easy you use the piss and dry technique let fe...</td>\n",
       "      <td>funny</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits   y\n",
       "0   0  honestly buffalo is the correct answer remembe...           hockey  11\n",
       "1   1  ah yes way could have been remember when he wa...              nba  14\n",
       "2   2  http youtu be 6xxbbr8isz0 40m49s if you didn f...  leagueoflegends  12\n",
       "3   3  he wouldn have been bad signing if we wouldn h...           soccer  16\n",
       "4   4  easy you use the piss and dry technique let fe...            funny   9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First time running do this\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(comment):\n",
    "    comment = comment.split()\n",
    "    comment = [stemmer.lemmatize(word) for word in comment]\n",
    "    comment = ' '.join(comment)\n",
    "    return comment\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    # Text preprocessing for the 'comments' column\n",
    "    #-------------------------------------------------------------\n",
    "    # Lowercase\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    # Remove all the special characters\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub(r'\\W', ' ', x))\n",
    "    # Remove all single characters\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    # Remove single characters from the start\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub(r'\\^[a-zA-Z]\\s+', ' ', x))\n",
    "    # Substituting multiple spaces with single space\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Lemmatization\n",
    "    df['comments'] = df['comments'].apply(lemmatize)\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    # Create a numerical class out of each possible subreddit\n",
    "    #-------------------------------------------------------------\n",
    "    df.subreddits = pd.Categorical(df.subreddits)\n",
    "    df['y'] = df.subreddits.cat.codes\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train = preprocess(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Converting Text to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Input X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_features**\n",
    "We set the max_features parameter to 1000, which means that we want to use 1000 most occurring words as features for training our classifier.\n",
    "\n",
    "**min_df** \n",
    "This corresponds to the minimum number of documents that should contain this feature. So we only include those words that occur in at least 5 documents. \n",
    "\n",
    "**max_df** \n",
    "Here 0.7 means that we should include only those words that occur in a maximum of 70% of all the documents. Words that occur in almost every document are usually not suitable for classification because they do not provide any unique information about the document.\n",
    "\n",
    "The fit_transform function of the CountVectorizer class converts text documents into corresponding numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=2000, \n",
    "    min_df=5, max_df=0.7, \n",
    "    stop_words=stopwords.words('english'), \n",
    "    binary=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(list(train['comments'])).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['y'].to_numpy()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the training data because 80/20 because the test file does not contain the true categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Bernouilli Naive Bayes (Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Naive Bayes performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3668571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home made Bernouilli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassBernouilliNB():\n",
    "    \n",
    "    def __init__(self, alpha): \n",
    "        self.alpha = alpha\n",
    "        self.X     = None\n",
    "        self.y     = None\n",
    "        \n",
    "        \n",
    "    def _compute_marginal_probabilities(self):\n",
    "        \"\"\" \n",
    "        Marginal probabilities for each class\n",
    "        \"\"\"\n",
    "        self.marginals = np.empty(len(self.classes))\n",
    "        for k in self.classes:\n",
    "            Y_k = self.data[self.data[:,-1] == k]\n",
    "            self.marginals[k] = float(Y_k.shape[0])/float(self.data.shape[0])\n",
    "        \n",
    "        \n",
    "    def _compute_priors(self):\n",
    "        \"\"\"\n",
    "        Compute the priors matrix P(Xj|Yk)\n",
    "        \"\"\"\n",
    "        self.priors = np.empty((self.n_features,len(self.classes),))\n",
    "        \n",
    "        for k in self.classes:\n",
    "            Y_k = self.data[self.data[:,-1] == k]\n",
    "            \n",
    "            for j in range(self.n_features):    \n",
    "                # number of times xj=1 and y=k\n",
    "                Xj1_Yk   = Y_k[Y_k[:,j] == 1.0]\n",
    "                \n",
    "                # Add Laplace smoothing (parameter self.alpha)\n",
    "                numerator   = self.alpha + Xj1_Yk.shape[0]\n",
    "                denominator = self.alpha*len(self.classes) + Y_k.shape[0]\n",
    "                theta_jk    = float(numerator)/float(denominator)\n",
    "                \n",
    "                self.priors[j,k] = theta_jk\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        For each class in y, compute the marginal probabilities P(Y=k)\n",
    "        For each feature Xj, compute the contional pronability P(Xj|Y)\n",
    "        \"\"\"\n",
    "        # Train set X and y\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.data = np.c_[self.X, self.y]\n",
    "        \n",
    "        # number of classes to predict\n",
    "        self.classes = set(self.y)\n",
    "        # number of features \n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # compute the marginal P(Y=k) and prior probabilities P(Xj|Y)\n",
    "        self._compute_marginal_probabilities()\n",
    "        self._compute_priors()\n",
    "        \n",
    "        # log both the marginal and prior probabilities for numerical stability\n",
    "        self.log_marginals   = np.log(self.marginals)\n",
    "        self.log_priors      = np.log(self.priors)\n",
    "        self.log_1minusprior = np.log(1.0 - self.priors)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Apply equation from lecture 10, slide 6 (matrix version),\n",
    "        to pick the most likely class\n",
    "        \"\"\"\n",
    "        self.X_test = X_test\n",
    "        \n",
    "        # Predictions proportional to likelihood for class 0 and class 1\n",
    "        self.likelihoods = np.empty((X_test.shape[0],len(self.classes),))\n",
    "        self.pred_class  = np.empty((X_test.shape[0],1,))\n",
    "        \n",
    "        for i in range(self.X_test.shape[0]):\n",
    "            # individual x to classify\n",
    "            x = X_test[i,:]\n",
    "            \n",
    "            for k in self.classes:\n",
    "                pred_yk  = self.log_marginals[k]\n",
    "                pred_yk += np.dot(x, self.log_priors[:,k])\n",
    "                pred_yk += np.dot((1.0-x), self.log_1minusprior[:,k])\n",
    "                \n",
    "                self.likelihoods[i,k] = pred_yk\n",
    "                \n",
    "            # predicted class corresponds to the maximum value of pred_yk for this instance\n",
    "            self.pred_class[i,0] = np.where(self.likelihoods[i,:] == np.amax(self.likelihoods[i,:]))[0][0]\n",
    "        return self.pred_class\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiClassBernouilliNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36742857142857144\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
